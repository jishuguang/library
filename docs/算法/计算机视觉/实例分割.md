# 实例分割

## 概述

## 论文

### 20220324 SparseInst

#### 1 概述

[Sparse Instance Activation for Real-Time Instance Segmentation](https://arxiv.org/abs/2203.12827)基于稀疏的Instance Activation Maps (IAM)来获取实例特征进行实例分割，并采用二部图匹配（bipartite matching）作Assignment避免了NMS后处理，取得了优异的推理性能和模型精度。

#### 2 主要内容

* 模型结构

SparseInst模型结构如下图所示，主要包括backbone、encoder、decoder三个部分：1) backbone采用ResNet等，保留C3-C5；2) encoder经Top-Down后，将P3-P5合并，其中C5额外通过PPM（Pyramid Pooling Module）以扩大感受野；3）decoder含有两个分支，instance分支基于IAM获取实例特征$z \in \mathbb{R}^{N \times D}$，进而通过3个FC预测实例类别、objectness（IoU）及mask kernel，其中kernel（$w \in \mathbb{R}^{N \times D}$）与mask分支提供的mask特征（$M \in \mathbb{R}^{D \times H \times W}$）结合预测实例mask，即$m = wM$。

decoder在encoder输出的基础上，添加了2个channel的location-sensitive features，即normalized absolute (x, y) coordinates。


![SparseInst](../../images/2022/sparse_inst.png)

* IAM

IAM的计算方式为$\textbf{A} = F_{iam}(\textbf{X}) \in \mathbb{R}^{N \times (H \times W)}$，其中$\textbf{X} \in \mathbb{R}^{D \times (H \times W)}$，$F_{iam}$采用$3 \times 3$卷积及sigmoid激活函数。IAM经normalize后，获取实例特征，即$z = \overline{\textbf{A}} \textbf{X}^\top \in \mathbb{R}^{N \times D}$。

为获取更多实例特征，可采用Group-IAM，即每个实例对应一组IAM（论文以4个作为一组），组内IAM得到的特征通过concatenating得到最终的实例特征。

* 训练

Ground Truth的Assignment策略采用二部图匹配，第$i$个预测实例（$m_i$表示预测mask）与第$k$个GT（$c_k$表示GT类别，$t_k$表示GT mask）的匹配Score定义为$C(i,k) = p_{i, c_k}^{1 - \alpha}\text{DICE}(m_i, t_k)^\alpha$，其中$\alpha$用于调节分类与分割的权重（论文取0.8），$\text{DICE}(m, t)=\frac{2\sum_{x,y}m_{xy}t_{xy}}{\sum_{x,y}m_{xy}^2 + \sum_{x,y}t_{xy}^2}$（式中$m,t$表示预测maks及GT mask在坐标$x,y$处的值）。

损失定义为$L = \lambda_cL_{cls} + L_{mask} + \lambda_sL_s$，其中：$L_{cls}$为分类损失，论文采用Focal Loss；$L_{mask}$为mask损失，论文采用$L_{mask} = \lambda_{dice}L_{dice} + \lambda_{pix}L_{pix}$综合dice loss及binary cross entropy loss；$L_s$为IoU对应的损失。

* 推理

前向推理获得$N$个实例的分类预测、objectness预测、mask预测，通过thresholding直接确定最终实例类别和mask，无需NMS和sorting。

#### 3 主要结果

* SparseInst模型精度和推理性能综合能力优于SOLO系列及YOLOLACT系列；
* 

#### 4 阅读小记

### 20200323 SOLOV2

[SOLOv2: Dynamic and Fast Instance Segmentation](https://arxiv.org/abs/2003.10152)

### 20191210 SOLO

[SOLO: Segmenting Objects by Locations](https://arxiv.org/abs/1912.04488)