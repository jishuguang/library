# 并发

## 概述

Approches to concurrency:
- multiple processes
- multiple threads

Why use concurrency:
- for separation of concerns
- for performance: task and data parallelism

## 线程管理

Basic thread managment
- Launch: `std::thread` constructor
- Wait or not：`join` or `detach`

Passing args
- copy
- std::move
- std::ref
- object pointer for member function

Thread ownership
- movable
- `std::jthread`

Thread amount
- `std::thread::hardware_concurrency()`

Identifying thread
- `std::thread::get_id()`
- `std::this_thread::get_id()`

## 线程共享数据

Problem
- race condition
- broken invariant

Avoid race condition
- wrap structure under protection guard
- refine structure using lock-free programming
- software transactional memory (STM)

### Mutex

Using mutex
- `std::mutex`
- `std::lock_guard`
    - `std::adopt_lock`

Avoid deadlock
- avoid nested lock
- avoid calling user-supplied code while holding a lock
- acquire lock in a fixed order
    - `std::lock`
    - `std::scoped_lock`
    - hand-over-hand locking
    - use a lock hierachy

`std::unique_lock`
- slight larger and slower than `std::lock_guard`
- use case
    - `std::defer_lock`
    - transfer ownership (moveable / not copyable)
    - flexible locking: doesn’t always own the mutex

Attention
- Don’t pass pointers and references to protected data outside the scope of the lock
- lock granularity
    - minimum necessary data
    - minimum possible time

### Alternative facilities

Protecting data during initialization
- problem: infamous double-checked locking
- `std::call_once, std::once_flag`
- static local variable

Protecting rarely updated data
- `std::shared_mutex`
    - exclusive locking: `std::lock_guard, std::unique_lock`
    - shared locking: `std::shared_lock`

Recursive locking (lead to sloppy thinking and
bad design)
- `std::recursive_mutex`

## 线程同步

### Waiting for an event with condition

condition variable
- `std::condition_variable`
- `std::condition_variable_any`
- need the `std::unique_lock` to lock/unlock internally

spurious wake
- code must be prepared
- minimal implementation: busy waiting

thread-safe queue with condition variable

### Waiting for one-off events

future
- `std::future` (only movable)
    - to share: `share` or `std::move`
- `std::shared_future`
    - need a copy of the shared_future object to each thread
- future objects themselves don’t provide synchronized accesses

return value from background task
- `std::async` retrun `std::future` holding the result
    - wait
    - get
- `std::launch::deferred, std::launch::async`

associating a task with a future
- `std::packaged_task` (callable object)
- `get_future` return `std::future`

making promise
- `std::promise<T>`
- `set_value`
- `get_future` return `std::future<T>` 

saving exception
- exception is stored in the future
- `some_promise.set_exception`

### Wait with a time limit

Clocks
- basic info
    - now
    - the type of the value
    - tick period: `std::ratio<60, 1>`
    - steady or not
- clock
    - system clock
    - steady clock
    - high resolution clock

Durations
- `std::chrono::duration<type, tick period>`
    - typedefs: nanoseconds, seconds, minutes, hours
- `std::chrono_literals`
- duration cast
    - implicit: do not require trunction
    - explicit: `std::duration_cast<>`
- `count`

Time points
- `std::chrono::time_point<clock, duration>`: time since epoch
- calculation
    - time_point with duration
    - time_point with time_point

Functions with timeout
- `std::this_thread::sleep_for/until`
- `std::timed_mutex.try_lock_for/until`
- ......

### Using synchronization to simplify code

Functional programming using future
- FP: the result of a function call depends solely on the parameters
- FP-Style parallel quicksort

Synchronizing operations with message passing
- CSP (Communicating Sequential Processes)
- Actor model

Continuation style
- feture.then(continuation)
- chain continuation
    - exception
    - future unwarpping

Waiting all or any
- when_all(begin, end)
- when_any(begin, end)

latches and barriers
- latch
    - count_down
    - wait
- barrier (reusable)
    - arrive_and_wait
- flex_barrier

## The C++ memory model and atomic operations

### Memory model

Objects and memory location
- object: a region of storage
- a object is stored in one or more memory locations
    - Every variable is an object, including those that are members of other objects.
    - Every object occupies at least one memory location.
    - Variables of fundamental types such as int or char occupy exactly one memory location, whatever their size, even if they’re adjacent or part of an array.
    - Adjacent bit fields are part of the same memory location.

Memory location and concurrency
- everything hinges on those memory locations. 
- if one or both of those accesses is not atomic, and if one or both is a write, then this is a data race.
- In order to avoid the race condition, there has to be an enforced ordering.

Modification order
- a modification order composed of all the writes to that object from all threads in the program.
- all threads in the system must agree on the order.
- don’t necessarily have to agree on the relative order of operations on separate objects.

### Atomic operations and types

atomic operations
- An atomic operation is an indivisible operation.
- `is_lock_free, is_always_lock_free`

`atomic_flag`
- `ATOMIC_FLAG_INIT` (initialization)
- `clear`
- `test_and_set`

`atomic<bool>`
- `store`
- `load`
- `exchange` (read-modify-write)
- `compare_exchange_weak/strong` (read-modify-write)
    - spurious failure: e.g. for machines that lack a single compare-and-exchange instruction

`atomic<T*>`
- `fetch_add/+=, fetch_sub/-=`
- `++, --`

atomic integral types
- `fetch_and, fetch_or, fetch_xor`

primary class template
- criteria: trivial copy-assignment operator, ...
- there are no atomic arithmetic operations on floating-point values

free functions for atomic operations
- `atomic_load, atomic_load_explict`

### Synchronizing operations and enforcing ordering

relationship
- synchronizes-with
- happens-before
    - strongly-happens-before (no memory_order_consume participation)

memory order
- `memory_order_seq_cst` (sequentially consistent ordering)
    - all threads must see the same order of operations
    - This constraint doesn’t carry forward to threads that use atomic operations with relaxed memory orderings
- `consume, acquire, release, acq_rel` (acquire-release ordering)
    - A release operation synchronizes-with an acquire operation that reads the value written
    - transitive
    - mix seq_cst & acq_rel: seq_cst behave like load_acq, store_rel, RMW_acq_rel
    - consume: dependency-ordered-before
- `relaxed` (relaxed ordering)
    - the only requirement is that all threads agree on the modification order of each individual variable

release sequence and synchronize-with
- the RMW chain of operations constitutes a release sequence and the initial store_rel synchronizes with the final load_acq.
- Any atomic read-modify-write operations in the chain can have any memory ordering.

fence
- `std::atomic_thread_fence` global operations
- if an acquire operation sees the result of a store that takes place after a release fence, the fence synchronizes with that acquire operation
- if a load that takes place before an acquire fence sees the result of a release operation, the release operation synchronizes with the acquire fence. 
- You can have fences on both sides.

ordering non-atomic operations
- sequence before
- high level mechanism provides ordering guarantees in terms of the synchronizes-with relationship

## lock-based concurrent data structure

### Guidelines for designing data structures for concurrency

guidelines
- thread safe
    - see broken invariants exclusively
    - avoid race conditions inherent in the interface
    - exception safe
    - restrict locks to avoid deadlock
- opportunity for concurrency (against serialization)
    - restrict the scope of locks
    - different locks for diffferent parts

### lock-based concurrent structures

thread-safe stack
- wrapper of std::stack

thread-safe queue
- wrapper of std::queue
- fine-grained locks for head and tail using a single-linked list

thread-safe map
- wrapper of std::map
- hash map using std::list bucket with shared_lock and unique_lock for each bucket
- fine-grained locks within each bucket using thread-safe list

thread-safe single-linked list
- fine-grained locks for each item and hand-over-hand locking for common operations 

## lock-free concurrent data structure

### definition and consequence

blocking and non-blocking
- blocking data structure: use mutex, cv, future with blocking calls
- nonblocking: without blocking call

more specific terms for nonblocking
- Obstruction-Free—If all other threads are paused, then any given thread will complete its operation in a bounded number of steps.
    - spin lock belongs to obstruction free
- Lock-Free—If multiple threads are operating on a data structure, then after a bounded number of steps one of them will complete its operation.
    - rely on the use of atomic operations and the associated memory-ordering
- Wait-Free—Every thread operating on a data structure will complete its operation in a bounded number of steps, even if other threads are also operating on the data structure.

pros and cons of lock-free data structures
- maximum concurrency
- robustness
- harder
- live lock
- may well decrease overall performance

### lock-free data structures

lock-free stack with single-linked list
- reclaim nodes when no threads in pop
- reclaim nodes using hazard pointers
- reclaim nodes using atomic shared_ptr
- reclaim nodes using manual reference count (internal/external count)

lock-free queue with single-linked list
- spsc
- mpmc
    - reclaim nodes using manual reference count (not lock free)
    - helping out another thread to avoid busy-waiting (lock free)

### guidelines for writing lock-free data structures

- use std::memory_order_seq_cst for prototyping
- use a lock-free memory reclamation scheme
    - pending deletion until no threads accessing
    - hazard pointers
    - reference counting
    - recycle nodes
- watch out for the ABA problem
    - common solution: include an ABA counter alongside the variable
- identify busy-wait loops and help the other thread

## Designing concurrent code

### Techniques for dividing work between threads

- dividing data between threads before processing begins
    - MPI (Message Passing Interface)
    - OpenMP
- dividing data recursively
- dividing work by task type
    - to seperate concerns
    - a sequence of tasks

### Factors affecting the performance of concurrent code

- the number of processors
- data contention and cache ping-pong
- false sharing
    - std::hardware_destructive_interference_size
- true sharing
    - std::hardware_constructive_interference_size
- oversubscription and excessive task switching

### Designing data structures for multithreaded performance

key factors: contention, false sharing, data proximity

- dividing array elements for complex operations
    - data that’s close together is worked on by the same thread
    - minimize the data required by any given thread
    - data accessed by separate threads is sufficiently far apart to avoid false sharing
- data access patterns in other data structures
    - binary trees
    - data with a mutex

### Additional considerations when designing for concurrency

- exception safety
    - async
    - packaged_task
- scalability and Amdahl's law
- hiding latency with multiple threads
- improving responsiveness with concurrency
    - event-driven

### Example

- parallel std::for_each
- parallel std::find
- parallel std::partial_sum
    - divide and forward
    - pairwise updates

## Advanced thread management

### Thread pools

- simple thread pool
- optimization
    - waiting for tasks submitted: function_wrapper + packaged_task
    - tasks that wait for other tasks

## 参考资料

- C++ Concurrency in Action 2nd Edition